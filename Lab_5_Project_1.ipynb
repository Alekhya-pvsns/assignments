{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MRJlvyNSPqvhfSt7MFpYjWlzaV-kI3RL",
      "authorship_tag": "ABX9TyNeNEaVVovRTppX9vJVKt9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alekhya-pvsns/assignments/blob/master/Lab_5_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setting up spark environment\n",
        "\n",
        "Project 1: Machine Learning Project without\n",
        "Mllib Pipline\n",
        "\n"
      ],
      "metadata": {
        "id": "-S-spCKfn80c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CxmcBRQRzag"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_spark_archive = '/content/drive/MyDrive/spark-3.0.1-bin-hadoop2.7.tgz'"
      ],
      "metadata": {
        "id": "VNxtSQfoTtmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf \"{path_to_spark_archive}\""
      ],
      "metadata": {
        "id": "Gvzj6tiMT0Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "-dGSIeJYT6lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Mc8v5LlPW3vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "path_to_spark = '/content/spark-3.0.1-bin-hadoop2.7'\n",
        "\n",
        "findspark.init(path_to_spark)\n",
        "\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I-4iw3TsXJ8o",
        "outputId": "1a30491e-8c7a-4486-d31f-dbc175331af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.0.1-bin-hadoop2.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Titanic Data\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "oHW9Sq7uXjuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName ('Titanic Data') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "AKc7gJsmYwUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read\n",
        "           .format(\"csv\")\n",
        "           .option(\"header\",\"true\")\n",
        "           .load(\"/content/drive/MyDrive/train.csv\"))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "ZdD61Gv_bJy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "dataset = df.select(col('Survived').cast('float'),\n",
        "                    col('Pclass').cast('float'),\n",
        "                    col('Sex'),\n",
        "                    col('Age').cast('float'),\n",
        "                    col('Fare').cast('float'),\n",
        "                    col('Embarked')\n",
        "                    )\n",
        "\n",
        "dataset.show(4)"
      ],
      "metadata": {
        "id": "9SNmXsP5brHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnull, when, count, col\n",
        "\n",
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "id": "2SPHjapXcXPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.replace('?', None)\\\n",
        "          .dropna(how='any')\n",
        "\n",
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "id": "Zw4ZiK5bczvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting categorical values into numerical values"
      ],
      "metadata": {
        "id": "qOEEaDV6dYXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show(3)"
      ],
      "metadata": {
        "id": "vZNd-6KSdMu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Sex',\n",
        "    outputCol='Gender',\n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "\n",
        "dataset = StringIndexer(\n",
        "    inputCol='Embarked',\n",
        "    outputCol='Boarded',\n",
        "    handleInvalid='keep').fit(dataset).transform(dataset)\n",
        "\n",
        "dataset.show(2)"
      ],
      "metadata": {
        "id": "gRotL5JOdfTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop('Sex')\n",
        "dataset = dataset.drop('Embarked')\n",
        "\n",
        "dataset.show(2)"
      ],
      "metadata": {
        "id": "8-tHQpCagMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering"
      ],
      "metadata": {
        "id": "PCQz7wyzgmvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "require_featured = ['Pclass', 'Age', 'Fare', 'Gender', 'Boarded']\n",
        "assembler = VectorAssembler(inputCols=require_featured, outputCol='features')\n",
        "transformed_data = assembler.transform(dataset)\n",
        "\n",
        "transformed_data.show(5)"
      ],
      "metadata": {
        "id": "5iXiDS1ggoyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "IexEe8Pohg9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])\n",
        "print(\"Number of train samples: \" + str(training_data.count()))\n",
        "print(\"Number of test samples:\" + str(test_data.count()))"
      ],
      "metadata": {
        "id": "Mg8jsWXYhiQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol='Survived',\n",
        "                            featuresCol='features',\n",
        "                            maxDepth=5)\n",
        "\n",
        "model = rf.fit(training_data)\n",
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "5nzBX8oliJIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "7bQn2lCMirEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol='Survived',\n",
        "    predictionCol='prediction',\n",
        "    metricName = 'accuracy')\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Training Accuracy = ', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaxmPLFEir8E",
        "outputId": "9d160e5a-9d0f-4e20-9dc4-3cc0aad61bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy =  0.8333333333333334\n"
          ]
        }
      ]
    }
  ]
}