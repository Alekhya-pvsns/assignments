{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SmCtlD71FaO6gI-N79uPHTo6TzDM_jBR",
      "authorship_tag": "ABX9TyPG6z2CtlSH2JeVPGdUAiyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alekhya-pvsns/assignments/blob/master/Lab_5_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project 2: Machine Learning Project with Mllib\n",
        "Pipline\n"
      ],
      "metadata": {
        "id": "awl2_DQcBQX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the pyspark environment"
      ],
      "metadata": {
        "id": "EH_OFEDZBoX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the JVM\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "2otVNAyRBSgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the spark archive\n",
        "\n",
        "path_to_spark_archive = '/content/drive/MyDrive/spark-3.0.1-bin-hadoop2.7.tgz'"
      ],
      "metadata": {
        "id": "2o7al1SwB8gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the archived file\n",
        "\n",
        "!tar xf \"{path_to_spark_archive}\""
      ],
      "metadata": {
        "id": "O3M4gsGjCOR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding environment variables\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "ev4J_-fdDbXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing findspark\n",
        "\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "bDOLP4tSDjie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing findspark, using the spark installation directory\n",
        "\n",
        "import findspark\n",
        "\n",
        "path_to_spark = '/content/spark-3.0.1-bin-hadoop2.7'\n",
        "\n",
        "findspark.init(path_to_spark)\n",
        "\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qwsEHrGiDy34",
        "outputId": "377f2178-abc3-4a89-a3e9-f39351952210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.0.1-bin-hadoop2.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Spark Session"
      ],
      "metadata": {
        "id": "rnGtX7fYEQYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing SparkSession\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Titanic Data\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Checking the session info\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "56xT4KVfEKrp",
        "outputId": "290b4e1e-a948-4330-e8b4-d55915b7a46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7891601026b0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://99e5c1c7c7e2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Titanic Data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data"
      ],
      "metadata": {
        "id": "6E5P5ujUEz55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read\n",
        "           .format(\"csv\")\n",
        "           .option(\"header\",\"true\")\n",
        "           .load(\"/content/drive/MyDrive/train.csv\"))\n",
        "\n",
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-YyjTOSEy7A",
        "outputId": "09b23354-4e15-4d6e-d00b-613e3edd77ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing functions"
      ],
      "metadata": {
        "id": "KeKdi8rMFEgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# Creating vectors from features\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Using Decision tree classifier\n",
        "from pyspark.ml.classification import RandomForestClassifier"
      ],
      "metadata": {
        "id": "XchuM2v1FG-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Pipeline"
      ],
      "metadata": {
        "id": "6EkcrZ0dF-j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pipeline\n",
        "\n",
        "from pyspark.ml import Pipeline\n"
      ],
      "metadata": {
        "id": "PrmUG0gwFVch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing and selecting columns\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "dataset = df.select(col('Survived').cast('float'),\n",
        "                    col('Pclass').cast('float'),\n",
        "                    col('Sex'),\n",
        "                    col('Age').cast('float'),\n",
        "                    col('Fare').cast('float'),\n",
        "                    col('Embarked')\n",
        "                    )\n",
        "\n",
        "dataset.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT8BTIhKODZD",
        "outputId": "c24f9d9b-ef09-4f37-9e66-f912f1547772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
            "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
            "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
            "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing null values\n",
        "\n",
        "from pyspark.sql.functions import isnull, when, count, col\n",
        "\n",
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H56DvM43OaOM",
        "outputId": "ff282fa0-1c76-4ff1-c7c3-d4e274d0d200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|177|   0|       2|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.replace('?', None)\\\n",
        "          .dropna(how='any')\n",
        "\n",
        "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxvykYydOhqJ",
        "outputId": "e422a011-0aaa-4a7d-b712-6e348615657a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+---+---+----+--------+\n",
            "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
            "+--------+------+---+---+----+--------+\n",
            "|       0|     0|  0|  0|   0|       0|\n",
            "+--------+------+---+---+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_df, test_df) = dataset.randomSplit([0.8,0.2], 11)\n",
        "print(\"Number of train samples: \" + str(train_df.count()))\n",
        "print(\"Number of test samples:\" + str(test_df.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVUQZPevGPfS",
        "outputId": "b610ae28-2633-41c0-854f-6d818f1bd08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples: 562\n",
            "Number of test samples:150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding without fit or transform\n",
        "\n",
        "Sex_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"Gender\")\n",
        "Embarked_indexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"Boarded\")\n",
        "\n",
        "# Assembling the features\n",
        "\n",
        "inputCols = ['Pclass', 'Age', 'Fare', 'Gender', 'Boarded']\n",
        "outputCol = \"features\"\n",
        "vector_assembler = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
        "\n",
        "# Modeling using Decision tree classifier\n",
        "\n",
        "dt_model = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")"
      ],
      "metadata": {
        "id": "YtnOOR-3Pq5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[Sex_indexer, Embarked_indexer, vector_assembler, dt_model])\n",
        "\n",
        "# Fitting the model\n",
        "\n",
        "final_pipeline = pipeline.fit(train_df)\n",
        "\n",
        "# Prediction on test data\n",
        "\n",
        "test_predictions_from_pipeline = final_pipeline.transform(test_df)\n",
        "test_predictions_from_pipeline.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bEbhsCzSgZB",
        "outputId": "9d2c4918-eeef-4119-af3e-84f25f29334a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+----------------------------------------+----------+\n",
            "|Survived|Pclass|Sex |Age |Fare   |Embarked|Gender|Boarded|features                           |rawPrediction                         |probability                             |prediction|\n",
            "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+----------------------------------------+----------+\n",
            "|0.0     |1.0   |male|19.0|263.0  |S       |0.0   |0.0    |[1.0,19.0,263.0,0.0,0.0]           |[11.225006325654135,8.774993674345865]|[0.5612503162827067,0.4387496837172932] |0.0       |\n",
            "|0.0     |1.0   |male|21.0|77.2875|S       |0.0   |0.0    |[1.0,21.0,77.2874984741211,0.0,0.0]|[10.846314240712049,9.153685759287951]|[0.5423157120356025,0.45768428796439753]|0.0       |\n",
            "|0.0     |1.0   |male|28.0|82.1708|C       |0.0   |1.0    |[1.0,28.0,82.1707992553711,0.0,1.0]|[10.937640547255747,9.062359452744255]|[0.5468820273627873,0.45311797263721276]|0.0       |\n",
            "|0.0     |1.0   |male|29.0|30.0   |S       |0.0   |0.0    |[1.0,29.0,30.0,0.0,0.0]            |[9.492896332649899,10.507103667350101]|[0.4746448166324949,0.5253551833675051] |1.0       |\n",
            "|0.0     |1.0   |male|29.0|66.6   |S       |0.0   |0.0    |[1.0,29.0,66.5999984741211,0.0,0.0]|[10.259357718972918,9.740642281027084]|[0.512967885948646,0.48703211405135416] |0.0       |\n",
            "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+----------------------------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Spark to load images as Dataframe"
      ],
      "metadata": {
        "id": "_RmhjGHnUsAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparkdl\n",
        "!pip install matplotlib scipy scikit-image\n",
        "\n",
        "!pip install tensorframes\n",
        "\n",
        "!pip install kafka-python image\n",
        "\n",
        "!pip install tensorflowonspark\n",
        "\n",
        "from sparkdl import readImages\n",
        "\n",
        "#image_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvg0-rjkUwG9",
        "outputId": "e3f96f1a-48d4-4ae0-ba05-3ac411b6bddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sparkdl in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tensorframes in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Collecting image\n",
            "  Downloading image-1.5.33.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from image) (9.4.0)\n",
            "Collecting django (from image)\n",
            "  Downloading Django-5.0.3-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from image) (1.16.0)\n",
            "Collecting asgiref<4,>=3.7.0 (from django->image)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from django->image) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from asgiref<4,>=3.7.0->django->image) (4.10.0)\n",
            "Building wheels for collected packages: image\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=4ecfa1b5edc1beb84ac64842424482ee83ed3f11ac9db1c1eeac932b89a870db\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/0c/a4/7cfa53a5c6225c2db2bfec08e782b43d0f25fdae2e995b69be\n",
            "Successfully built image\n",
            "Installing collected packages: asgiref, django, image\n",
            "Successfully installed asgiref-3.7.2 django-5.0.3 image-1.5.33\n",
            "Requirement already satisfied: tensorflowonspark in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflowonspark) (23.2)\n",
            "Requirement already satisfied: setuptools>38.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowonspark) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparkdl import readImages\n",
        "image_df = readImages(sample_img_dir)\n",
        "image_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "hCxovVc8tKXv",
        "outputId": "36802374-c8d7-4066-8158-da136eb2811b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sample_img_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6af810fc129b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_img_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from sparkdl import DeepImageFeaturizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\n",
        "p = Pipeline(stages=[featurizer, lr])\n",
        "\n",
        "p_model = p.fit(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "9YkQytcQX2Z7",
        "outputId": "7b88ed4b-15be-4d89-9d0e-79077a0de338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bd2717126cc7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimage_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeaturizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepImageFeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"InceptionV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_df' is not defined"
          ]
        }
      ]
    }
  ]
}